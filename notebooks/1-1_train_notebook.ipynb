{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from efficientv2_unet.model.efficientv2_unet import create_and_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model (and evaluate)\n",
    "\n",
    "A simple example how to train an EfficientV2-UNet model.\n",
    "\n",
    "Provide a folder with images (RGB), a folder with masks (where background has a value of 0 and foreground a value of 1), and the type of EfficientV2-Net model to build your model from.\n",
    "\n",
    "Images and corresponding masks must be placed in separate folders and must have identical names, e.g.\n",
    "\n",
    "```\n",
    "path/to/images:\n",
    "                - image01.tif\n",
    "                - image02.tif\n",
    "                - ...\n",
    "path/to/masks:  - image01.tif\n",
    "                - image02.tif\n",
    "                - ...\n",
    "```\n",
    "\n",
    "A model will be trained by randomly splitting the input images and masks into 70% training images, 15% validation images and 15% test images.\n",
    "\n",
    "For custom splitting of your training data, check the `1-2_split_data_and_train` notebook\n",
    "\n",
    "The splitting will move the images into sub-folders, giving you:\n",
    "\n",
    "```\n",
    "├── path/to/images\n",
    "    ├── train\n",
    "    ├── val\n",
    "    └── test\n",
    "└── path/to/masks\n",
    "    ├── train\n",
    "    ├── val\n",
    "    └── test\n",
    "```\n",
    "\n",
    "Your input images, will be patched into smaller images (which are saved to file into corresponding 'crop' folders). This is done at the native resolution, but also at half, and 1/3 resolution. Hence, you can train with rather big images.\n",
    "\n",
    "The training includes batch image autmentation, including:\n",
    "- HorizontalFlip\n",
    "- RandomRotate90\n",
    "- RandomGamma\n",
    "- RandomBrightnessContrast\n",
    "- ElasticTransform\n",
    "- GridDistortion\n",
    "- OpticalDistortion\n",
    "- RandomSizedCrop\n",
    "\n",
    "Monitored metrics include:\n",
    "- BinaryAccuracy (at a threshold of 0.5)\n",
    "- BinaryIoU (at a threshold of 0.5)\n",
    "\n",
    "Training a model will always load the image-net weigts at the beginning.\n",
    "\n",
    "At the end of the training, the final epoch model and the best-checkpoint models are saved to file.\n",
    "\n",
    "And your models will also be evaluated on the test dataset, at full, half and 1/3 resolution. The console will print the best threshold, with the best resolution to use.\n",
    "\n",
    "Eventually, in the model folder, there will be a json file containing all the training and evaluation metadata. Alongside with some graphs, showing\n",
    "the models' performance for different thresholds on the test images at different resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "image_folder = 'path/to/images'     # folder with the images\n",
    "mask_folder = 'path/to/masks'       # folder with the corresponding masks [0=background, 1=foreground]\n",
    "\n",
    "efficientnet_basemodel = 'b0'       # any of ['b0', 'b1', 'b2', 'b3', 's', 'm', 'l']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the next cells, you can monitor the training also in tensorboard.\n",
    "For that open another cmd of your environment,\n",
    "- cd to your basedir\n",
    "- start tensorboard with: \"tensorboard --logdir=.\"\n",
    "- (or \"tensorboard --logdir={basedir}\" if you did not cd to the basedir)\n",
    "- then, access tensorboard in a browser: http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_and_train(\n",
    "    name=None,                              # if not specified it is named 'myEfficientUNet_<efficientnet_basemodel>'\n",
    "    basedir='path/to/saving_location',      # if not specified it will be placed in the current wordking directory\n",
    "    train_img_dir=os.path.abspath(image_folder),\n",
    "    train_mask_dir=os.path.abspath(mask_folder),\n",
    "    efficientnet=efficientnet_basemodel,\n",
    "    epochs=100,                             # default\n",
    "    batch_size=64,                          # default\n",
    "    file_ext='.tif'                         # default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters explained\n",
    "\n",
    "**name**: you can chose any name for the model (all training metadata will be saved as a readable json file into the model folder). Evetually, after the training, you model will be save like, e.g.:\n",
    "```\n",
    "basedir / models / name :\n",
    "                          - name.h5  # EfficientV2UNet model file\n",
    "                          - name_best-ckp.h5  # Best EfficientV2UNet model file according to validation metrics\n",
    "                          - name.json  # model training and evaluation metadata\n",
    "                          - name*.png's  # several graphs showing the models' performance at different thresholds\n",
    "```\n",
    "\n",
    "**epochs**: number of iterations over the train/val data.\n",
    "\n",
    "**batch_size**: batch size, i.e. how many images belong to the same batch. That's a parameter you could decrease if you run into memory issues, but must be a multiple of 16.\n",
    "\n",
    "**file_ext**: image file extension. Tifffile is used to read the images, so I strongly suggest using '.tif'. (png was not tested)\n",
    "\n",
    "\n",
    "### Other parameters\n",
    "\n",
    "string inputs for validation image and masks, and test image and mask folders: You can supply those, if you have already split your images. *The sum of validation and test images must be below 80% of all images*\n",
    "\n",
    "**early_stopping**: by default False. Enabled, it will stop the training if there is not major change in the validation's BinaryIoU metric.\n",
    "\n",
    "**img_size**: default is 256. Is the size of image patches the model will be traied on.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
